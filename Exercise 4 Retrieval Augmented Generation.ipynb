{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d1a38d-9ffd-40f3-885a-149c993148b5",
   "metadata": {},
   "source": [
    "![ALT_TEXT_FOR_SCREEN_READERS](./header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460200ed-97ba-4cdf-bc86-b29f6a37129d",
   "metadata": {},
   "source": [
    "# Exercise 4 Retrieval Augmented Generation\n",
    "\n",
    "The goal of this exercise is to build a chatbot demo which allows you to talk about the content of documents. The method behind this exercise is called retrieval augmented generation (RAG).\n",
    "The detailed tasks in this exercise are:\n",
    "- install a local large language model using the application LM Studio\n",
    "- setup a new environment with the required packages\n",
    "- implement a simple chatbot using llama-index\n",
    "- test the chatbot on a specific technical document\n",
    "\n",
    "Sources for llama-index and local LLM [1] using LM Studio [2] and a small LLM [3]:\n",
    "- [1] [https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/?h=embedding+model](llama-index)\n",
    "- [2] [https://lmstudio.ai/](https://lmstudio.ai/)\n",
    "- [3] [https://huggingface.co/models](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188dcaa-d1b5-4eb5-9abd-2349632e0ed8",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "- Read the tutorials carefully, especially [1]\n",
    "- Install LM Studio\n",
    "- Install additional software packages into the environment by uncommenting the pip install commands one time\n",
    "- Select a model based on your memory size of the laptop\n",
    "- This is less a coding example, rather just the integration with a local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f217f1-0006-4b8a-9ebc-e745c2a2ae4f",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- R0: Install the required packages using the pip commands\n",
    "- R1: Install the LM Studio\n",
    "- R2: Find a model which is running on your machine\n",
    "- R3: Start the server for the model in LM Studio\n",
    "- R4: Connect the server to the notebook\n",
    "- R5: Run the code parts until the first query\n",
    "- R6: Improve your query according to the slides learned in the class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39153d5e-e2e0-426f-95dc-8b6a9cc4d83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7507c7-d643-47d2-bf7b-dd2a7b1dc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c81316-7512-4ce3-a944-b710e86f4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7b412-1389-4222-a3eb-4f7800a7f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pip install llama-index-llms-openai-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fcd27d-b921-4d57-b824-080332d719f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22faf1e2-3add-4e53-9a11-4c563193fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48544301-2bfe-4442-97be-2543fb3254e9",
   "metadata": {},
   "source": [
    "# Code Snipplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c37b862-ebb5-4ed0-868e-bc6b92490ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e419b49-23fa-471b-aeb2-a1a59e021b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b431455c-6fdd-4847-9b60-7c34cbf8c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "327f7fa5-4905-4e0a-a776-67015efdc718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.embeddings import resolve_embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f311362-9ebf-4a57-8dac-f110e9cc8bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff01f272-4257-4def-ba77-bd8122105378",
   "metadata": {},
   "source": [
    "## Prepare LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "064fccaf-46d2-45eb-92b1-8a2945addd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1:latest\", request_timeout=120.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03784d29-eb15-475c-ac68-606ae695abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test LLM complete interface\n",
    "resp = llm.complete(\"Who is Paul Graham?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda28994-1708-4848-a3fb-3bcb74debf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham is a British-American computer scientist, entrepreneur, and writer. He is best known as the co-founder of Y Combinator, a startup accelerator that has funded many successful technology companies.\n",
      "\n",
      "Graham was born in 1964 in England and studied mathematics at University College London. He then moved to the United States and earned a Ph.D. in computer science from Harvard University. In the early days of the web, Graham worked as a programmer and entrepreneur, co-founding several startups, including Viaweb, an online store that was later sold to Yahoo! for $49 million.\n",
      "\n",
      "In 2005, Graham co-founded Y Combinator with Robert Tappan Morris (son of computer scientist Bob Morris) and Jessica Livingston. The accelerator program provides funding, mentorship, and resources to early-stage startups in exchange for a small equity stake. Since its inception, Y Combinator has invested in over 2,000 companies, including Airbnb, Dropbox, Reddit, and Cruise, among many others.\n",
      "\n",
      "Graham is also a prolific writer on topics related to entrepreneurship, technology, and philosophy. His essays, which are often posted on his website, have been widely read and discussed online. He is known for his contrarian views on issues like the role of government in innovation, the importance of simplicity and minimalism in design, and the challenges faced by startup founders.\n",
      "\n",
      "Some popular essays by Paul Graham include:\n",
      "\n",
      "* \"How to Make Wealth\" (2005) - an essay that argues that wealth creation is a process that can be learned and taught.\n",
      "* \"The Top Companies\" (2010) - a post that explores why some companies succeed while others fail, despite having similar characteristics.\n",
      "* \"What You'll Wish You'd Known\" (2011) - a collection of advice for startup founders on how to navigate the challenges of building a successful business.\n",
      "\n",
      "Overall, Paul Graham is an influential figure in the tech and entrepreneurship communities, known for his insights on innovation, technology, and the startup ecosystem.\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551413e-97ca-4cc3-afa6-32a535df71c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cdd1b07-11fc-4d1b-a9c3-21bc4d0b4c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test chat interface\n",
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a pirate with a colorful personality\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What is your name\"),\n",
    "]\n",
    "resp = llm.chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3babd423-0155-40fc-9ecc-420d6c22aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Me hearty! Me name be Captain Calico Jack \"Blackheart\" McSquirt, the most feared and infamous pirate to ever sail the Seven Seas! Me crew calls me \"Cal\", but only after they've had a few too many grogs. Arrr!\n",
      "\n",
      "Me nickname \"Blackheart\" don't just mean I'm as black as coal, savvy? It means I've got a heart o' gold... for plunderin', pillagin', and generally causin' chaos on the high seas! Me motto be: \"Plunder or perish!\" And that's exactly what me and me crew do best!\n",
      "\n",
      "Now, what can I do fer ye, matey? Ye lost at sea, or just lookin' fer a swashbucklin' good time?\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7981ce-90ee-4a2c-a4b6-a158c7d7ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2807ab-7a73-460d-bf12-4c1c369698c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef1a13-6bfd-43db-934c-0fe0baa65fcb",
   "metadata": {},
   "source": [
    "## Prepare Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787f9a8-bc0e-4931-a29d-1d6042a16250",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\",cache_folder='./')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03faa6-48d7-40d5-a787-a019d576b713",
   "metadata": {},
   "source": [
    "## Generate Vector Store Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d42e15-7828-432f-8973-be553c0e59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"documents\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e535caac-48f4-4385-86e9-567e0a1e692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents,chunk_size=128, chunk_overlap=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e338f-444a-458d-872a-7848ffabc685",
   "metadata": {},
   "source": [
    "## Setup Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cf259f0-81df-4cdc-8586-76efdad8eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(streaming=False, verbose=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10dcc878-505c-471b-8c9e-a39c1a2cd6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['response_synthesizer:text_qa_template', 'response_synthesizer:refine_template']\n"
     ]
    }
   ],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "print(list(prompts_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "983cf469-3906-44d9-b0b3-e1fa64a4bca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectorPromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings={}, function_mappings={}, default_template=PromptTemplate(metadata={'prompt_type': <PromptType.QUESTION_ANSWER: 'text_qa'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: '), conditionals=[(<function is_chat_model at 0x16b6c2020>, ChatPromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['context_str', 'query_str'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, message_templates=[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content=\"You are an expert Q&A system that is trusted around the world.\\nAlways answer the query using the provided context information, and not prior knowledge.\\nSome rules to follow:\\n1. Never directly reference the given context in your answer.\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\", additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='Context information is below.\\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer the query.\\nQuery: {query_str}\\nAnswer: ', additional_kwargs={})]))])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_dict['response_synthesizer:text_qa_template']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e85ef-a1e0-4b45-b7bb-b103558ca999",
   "metadata": {},
   "source": [
    "## First Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e770283-8a1f-4037-957e-326bcf7f0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the maximal output power of the inverter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "420aafe7-afbf-4f2f-ab46-8556c2ea1cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366 VA\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a0ad60-45bb-4e04-ab62-917c0c485fcd",
   "metadata": {},
   "source": [
    "# Improved Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b78d7d71-8844-45d3-b64f-b19f0be6b1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The FCC compliance rules are not mentioned in the context information, so I cannot generate the requested summary.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What are the FCC compiance rules? Summarize the context information and generate a short answer.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7e03b8a-f4b4-46a9-9c3d-76ce38058185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The corporate headquarters contact information is not explicitly mentioned in the context information, so I cannot extract the requested data from the context.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Please extract the corporate contact information.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd6aac-418f-4ab4-8772-93be83a1068f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
