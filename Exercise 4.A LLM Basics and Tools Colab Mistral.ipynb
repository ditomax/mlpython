{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54c5f04-16c9-4bc8-8458-3b870044f330",
   "metadata": {},
   "source": [
    "![ALT_TEXT_FOR_SCREEN_READERS](./header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdb1b5-979d-4b1a-b26e-1bd48859d405",
   "metadata": {},
   "source": [
    "# Exercise 4.A LLM Basics and Tools using Colab and Mistral LLM\n",
    "\n",
    "The goal of this exercise is to setup a connection from the notebook to a local or remote large language model (LLM). Using this connection\n",
    "the basic modes of working with LLMs shall be tested:\n",
    "- Text Completion: complete a text. A query text is given and the model continues this text.\n",
    "- Chat: A list of system and user messages is given and the model generates the next message.\n",
    "\n",
    "We are using **Mistral.ai**[1] for cloud execution of the LLM and the framework **langchain**[2] for the access to the model.\n",
    "\n",
    "- [1] [https://mistral.ai//](https://mistral.ai/)\n",
    "- [2] [https://www.langchain.com/](https://www.langchain.com/)\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ditomax/mlpython/blob/main/Exercise%204.A%20LLM%20Basics%20and%20Tools%20Colab%20Mistral.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ab026-4589-42e9-b8dc-1c8c689485de",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "- Read the tutorial carefully\n",
    "- Retrieve a Mistral API Key [1] \n",
    "- Install **langchain_mistralai** and **langchain** packages into your environment (use pip inside the workbook and comment it out later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062af60-c1a6-45f6-ad04-4fa4a3e6ce40",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- R0: Instantiate a text completion model object and improve the head of the story for completion (10%)\n",
    "- R1: Instantiate a chat completion model object and work to improve the system prompt message (10%)\n",
    "- R2: Test the effect of the temperature parameter values 0.0 and 1.0 in the chat completion example (20%)\n",
    "- R3: Extend the structured output of the joke class (20%)\n",
    "- R4: Check the produced tool message for correct parameters (10%)\n",
    "- R5: Extend the tool calling agent by a system date and time tool (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c81e1-b46f-4c1d-a65e-73026116b3c7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d7a97-5801-4d83-8070-ee6169b88d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain_mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9898a6c-7d46-43f1-a121-4d898052b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa52684-85c1-498b-9260-3ec9c75672b4",
   "metadata": {},
   "source": [
    "# Text Completion\n",
    "\n",
    "Text completion means that the LLM takes the given text as start of a story and continues to generate tokens which extend the story.\n",
    "Such a model interface takes a string as input and generates a string as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a4ed0-4459-473e-a713-eeb10c574f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_mistralai import ChatMistralAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbd1a6b-df35-4beb-aea8-565bace5af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MISTRAL_API_KEY\"] = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c207790-932c-4751-a55a-d4e56d416787",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small-2506\",temperature=0,max_retries=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83457efe-3a93-4bc5-831e-1c848571ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R0: improve the start of the story to resemble your favorite fairy tale...\n",
    "#\n",
    "result = llm.invoke('Once upon a time, in a dark tyrolian valley ')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041175af-c5fc-41fc-ae3b-469a69e60966",
   "metadata": {},
   "source": [
    "# Chat Completion\n",
    "\n",
    "A chat completion model takes a list of messages as input and generates the next message. The resulting message is of type **AIMessage**.\n",
    "The list of messages can contain tuples with role and prompt for different message objects from the langchain_core.messages module.\n",
    "\n",
    "**Note:** The chat model is a different interface compared to the completion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d7a4d5-9377-4b05-a285-bdba55302da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4f695-89c5-4706-8bca-4c59b0d1271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small-2506\",temperature=0,max_retries=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6963eb-566b-4d21-afab-7c677713befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R1: improve the system prompt such that it contains all elements as learned in the slides.\n",
    "#\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a skilled philospher. You can explain complext information in simple terms.\" ),\n",
    "    (\"human\",  \"What is the meaning of life?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b9122-d310-43e0-9f62-b9812ce039a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the LLM with the message\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2ba92-738c-4702-b5a2-f50aadd8e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific return value\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2236b-c06b-42d6-810a-5ab8383c4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other metadata\n",
    "print(ai_msg.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe23026-b446-421d-84a8-a9d92641f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R2: test the impact of the temperature parameter (1.0) using the same prompt.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ade5d2-ac9a-4d45-961e-ba450f06128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ea9f3e-a189-4121-9bd1-38a930ef029f",
   "metadata": {},
   "source": [
    "# Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab15ba-e350-4b55-a2c3-453f43fe0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa48132-51c6-4da6-814b-abb40e1d7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(description=\"Detailed explanation why the joke if funny.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78861de-169d-48a5-bf98-5f5c8fce3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small-2506\",temperature=0,max_retries=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df54425-bd85-4f3e-be86-bfbff4c7617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a30f7-80ff-4904-bc40-4dd36a38015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_msg = structured_llm.invoke(\"Tell me a joke about AIs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a07b4-929a-4c16-8dd6-fbcbb95789ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d971280-1ccd-46a1-aefe-45635014363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R3: extend the structured output by an explanation field which contains the reasoning, why this is actually funny.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd8ec7-aede-4312-ad4c-d009c3f9f1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f6c668-fcdb-4324-b906-3c799a3a3e3e",
   "metadata": {},
   "source": [
    "# Tool use\n",
    "\n",
    "Tool use is a way to extend the function of LLMs. This is done in the following steps:\n",
    "1. in the first step, the tools are defined and the definitions are passed to the LLM as context information. The LLM is instructed to extract optional tool use, including tool name and tool parameters, if the user query requests a tool use. In this case the LLM generates a tool use message with all required parameters.\n",
    "1. in the second step, a piece of application logic takes the tool call messages and executes the tool calls. The results are placed back in the message list.\n",
    "1. in the last step, the tool results are used to formulation the answer.\n",
    "\n",
    "Those steps are usually stitched together in an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443c62d-c8c1-4aca-a92c-0ee25dda6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171c51e-0e45-4066-8110-bd3604bf3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# @tool is an annotator which adds specific properties to your tool function. Those properties allow the LLM to understand the parameters and the output of the tool.\n",
    "#\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d4d1a2-390f-40e6-a9c3-611aacb07044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def square_root(x: float) -> float:\n",
    "    \"\"\"returns the square root of x.\"\"\"\n",
    "    return math.sqrt(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426eec74-4ec4-45f2-87d4-f99bc2b23dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call tool for testing\n",
    "print(multiply.invoke({\"first_int\": 4, \"second_int\": 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383956a-afc2-4b31-abcb-152ee215051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call tool with wrong parameters for testing\n",
    "try:\n",
    "    print(multiply.invoke({\"first_int\": 4.1, \"second_int\": 5}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e1786-5e7d-4f76-841f-336bbe638ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(square_root.invoke({'x':25.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a28058-1e2b-4768-8645-2d6fed258aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd0433-d1ac-4b22-9473-628e8f8aeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare LLM for tool use\n",
    "#\n",
    "tools = [multiply,square_root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7385bc-485f-47d3-94b9-b2604298f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small-2506\",temperature=0,max_retries=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d9995-4616-4fea-a724-bcff3e1a90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# bind tools to LLM\n",
    "#\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4456c55-4ecc-412e-94e0-8396c21f5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is ten times 7ish? Use tool calling.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28707264-8395-4a64-bb42-af485c97c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R4: check the resulting tool message for the correct tool parameters. Extract the tool calling values (tool name, tool parameters). Those parameter can be used to call the python tool.\n",
    "#\n",
    "result = llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee9368f-16a4-428c-ae3d-990f5d839a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319caba3-264f-4c56-bf4a-620b96290c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaea2c-525d-4186-8206-738238873a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743b9c3b-4aa5-4ca8-8f28-b241c5aad38a",
   "metadata": {},
   "source": [
    "# Tool calling agent\n",
    "\n",
    "The langchain tool calling agent is a class which integrates all three steps of tool calling into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b35e7e-da01-40cb-9894-616aa852373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc6844-3ca8-49de-bb6f-3b008aa970a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful calculation assistant. Use your tools where it is appropriate. Mimic the style of the user query.\"), \n",
    "    (\"human\", \"{input}\"), \n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894621d-4287-4357-92ab-d3167fbec48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small-2506\",temperature=0,max_retries=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d41447-b25d-433a-a7ff-147a3c004b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6b07b2-91f1-47a4-9b33-77e3428010da",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34540d74-8b37-440c-be34-1589cf75be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_executor.invoke({\"input\": \"what is the square root of ten times 7ish?\", }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa566f9-f348-4b33-ad5c-fa1c29fe674b",
   "metadata": {},
   "source": [
    "# Agent with system time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f060be2-07c3-482e-bb70-ca384bf0be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R5: extend the tool calling agent by a tool which returns the current date and time as string. The LLMs usually do now know date and time!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dfd2e-9623-4edc-a329-318620b21f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157d512-8e70-4433-99a6-d818426c6a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
