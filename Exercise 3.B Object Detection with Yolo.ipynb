{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d1a38d-9ffd-40f3-885a-149c993148b5",
   "metadata": {},
   "source": [
    "![ALT_TEXT_FOR_SCREEN_READERS](./header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460200ed-97ba-4cdf-bc86-b29f6a37129d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exercise 3.B Object Detection with Yolo v11\n",
    "\n",
    "The goal of this example is to implement a modern object detection model with a custom dataset. The following steps are performed:\n",
    "\n",
    "- Installation of the yolo v8 framework on local machine or in colab\n",
    "- Test of installation\n",
    "- Preparation of custom training data in Roboflow\n",
    "- Training of yolo on custom training data\n",
    "- Evaluation on custom test data\n",
    "- Analysis\n",
    "- Enhancement step\n",
    "\n",
    "The dataset used will be created or extended by you during the exercise. So be prepared to create your own images.\n",
    "\n",
    "We will use yolo v11 [2], a reimplementation of yolov5 [1][3] in pytorch [4]. In detail we will follow the tutorial for training of a custom dataset as described here [5].\n",
    "\n",
    "Sources for yolov8 and the labeling tools:\n",
    "- [1] [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5)\n",
    "- [2] [https://github.com/ultralytics/ultralytics?tab=readme-ov-file](https://github.com/ultralytics/ultralytics?tab=readme-ov-file)\n",
    "- [3] [https://arxiv.org/pdf/2004.10934.pdf](https://arxiv.org/pdf/2004.10934.pdf)\n",
    "- [4] [https://github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)\n",
    "- [5] [https://blog.roboflow.com/yolov11-how-to-train-custom-data/](https://blog.roboflow.com/yolov11-how-to-train-custom-data/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5159014-d15c-4cc0-a794-e30843cf987c",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "- Read the tutorials carefully, especially [5]\n",
    "- Use at least 3 classes of objects\n",
    "- Create at least 100 images of each class (use your mobile phone for image generation)\n",
    "- Use the roboflow platform for labeling (free license model available)\n",
    "    - use the box annotation only.\n",
    "    - you can share the annotation task with other students (up to three per team) \n",
    "- Use a train/test split of 80/20\n",
    "- Download the dataset version as a ZIP from Roboflow. Place the content in your project folder where you start the training. You will have to adapt the path information in the data.yaml file of the dataset. E.g.\n",
    "```\u001b\n",
    "path: <absolute path to your data folder>\n",
    "train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "nc: ...\n",
    "names: [...]\n",
    "```\n",
    "\n",
    "- For training either use your notebook, or try a colab instance if your memory or CPU power does not support the model training\n",
    "- Start the training with the smallest model version YOLOv8n (nano)\n",
    "- Add the following additional parameters:\n",
    "    - plots=true\n",
    "    - project=myproject (this will give you the training results in the subfolder myproject).\n",
    "- A possible command line call for training is:\n",
    "```\n",
    "(yolov8) :-> yolo task=detect model=yolov8n.pt data=./mydata/data.yaml epochs=5 imgsz=640 mode=train plots=true project=myproject\n",
    "``` \n",
    "- If you want to continue training your model, you can use a path to your last model as the model parameter (e.g. model=./myproject/train6/weights/best.pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124388cb-7f8b-465c-967d-221fe08e6ea4",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- R0: Install yolo into your conda environment using pip (10%)\n",
    "- R1: Collect images of objects and create your dataset in roboflow (20%)\n",
    "- R2: Train a nano model on your dataset. Use either command line or python binding (20%)\n",
    "- R3: Analyse the training results (confusion matrix) and find the class with the worst performance (10%)\n",
    "- R4: Add 50 new images of the worst class (10%)\n",
    "- R5: Document the improvement after the update of the training data (10%)\n",
    "- R6: Include the confusion matrices of the R3 and R5 stages into the notebook (10%)\n",
    "- R7: Include a validation picture as proof. This can be found in the results folder (e.g. myproject/train6/val_batch0_pred.jpg) (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528ea6e-9d9d-4c4e-92d5-b553d32a1ff5",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| Requirement  | mAP@50 all classes  | Comment | \n",
    "|---|---|---|\n",
    "| R3  | mAP@50 first training  | worst class  |\n",
    "| R5  | mAP@50 second training  | ...  | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524f6a0-85f1-45fa-99d0-7a6679498545",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00e1f6-931c-4830-9bbe-14408d4f0cfd",
   "metadata": {},
   "source": [
    "![place your image here](./confusion1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23d159-50b7-4e52-882f-7e2194270fbb",
   "metadata": {},
   "source": [
    "## Validation Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a028f1-0022-4854-adaa-bb7aefa5b920",
   "metadata": {},
   "source": [
    "![place your image here](./validation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc9aa5b-c949-4652-9122-26f568c928a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
