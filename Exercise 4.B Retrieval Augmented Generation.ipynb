{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d1a38d-9ffd-40f3-885a-149c993148b5",
   "metadata": {},
   "source": [
    "![ALT_TEXT_FOR_SCREEN_READERS](./header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460200ed-97ba-4cdf-bc86-b29f6a37129d",
   "metadata": {},
   "source": [
    "# Exercise 4.B Retrieval Augmented Generation\n",
    "\n",
    "The goal of this exercise is to build a chatbot demo which allows you to talk about the content of documents. The method behind this exercise is called retrieval augmented generation (RAG).\n",
    "The detailed tasks in this exercise are:\n",
    "- install a local large language model using the application Ollama\n",
    "- setup a new environment with the required packages\n",
    "- implement a simple chatbot using langchain[2]\n",
    "- test the chatbot on a specific technical document\n",
    "- test the impact of the embedding model and the augmentation on the quality of the results\n",
    "\n",
    "We are using Ollama[1] for local execution of the LLM and the framework langchain[2] for the access to the model.\n",
    "\n",
    "- [1] https://ollama.com/\n",
    "- [2] https://www.langchain.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8188dcaa-d1b5-4eb5-9abd-2349632e0ed8",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "- Install Ollama on your computer\n",
    "- Install additional software packages into the environment by uncommenting the pip install commands one time\n",
    "- Select a model based on your memory size of the laptop\n",
    "- This is less a coding example, rather just the integration with a local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f217f1-0006-4b8a-9ebc-e745c2a2ae4f",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- R0: Install the required packages using the pip commands\n",
    "- R1: Install the Ollama software\n",
    "- R2: Find a model which is running on your machine\n",
    "- R3: Start the server for the model\n",
    "- R4: Connect the server to the notebook\n",
    "- R5: Run the code parts until the first query\n",
    "- R6: Test the RAG function with and without supporting context for the huggingface embedding\n",
    "- R7: Test the RAG function with and without supporting context for the ollama embedding\n",
    "- R8: Test the RAG function with an easy and one difficult query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13dd8e-6749-44cc-bd3a-91d13c6aa015",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5acf3f-5823-46ae-8ee1-aec759e58111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e545cac-6de0-47d9-b97d-83edab49b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5431b-56c1-42b9-a926-27a89d554fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7137d-562b-41b2-ab3b-7001a752eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553f8ec-bbb2-4cc5-926b-bd07eaddb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297909e-daa3-4a2d-8430-8894cd47f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -qU ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48544301-2bfe-4442-97be-2543fb3254e9",
   "metadata": {},
   "source": [
    "# Import Basic Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27dee76-13ce-4cc8-9f31-08eda0f0ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef1a13-6bfd-43db-934c-0fe0baa65fcb",
   "metadata": {},
   "source": [
    "# Prepare Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787f9a8-bc0e-4931-a29d-1d6042a16250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load embeddings model (we will test two different models R6, R7)\n",
    "#\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# R6\n",
    "embeddings = OllamaEmbeddings(model=\"llama3\")\n",
    "\n",
    "# R7\n",
    "#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057ad8b-6bfa-400d-8e08-f314c0bf0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Test embedding model\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeec0d3-b15d-43c0-8edf-20efa0332560",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_embedding = embeddings.embed_query(\"Woman\")\n",
    "print(str(single_embedding)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4487826-37e3-418b-9172-5ca962ef3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_embedding = embeddings.embed_query(\"Man\")\n",
    "print(str(single_embedding)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7526f-9e78-4ddb-afab-dff1be90a300",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(single_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396f6ea-ccfa-423c-9199-77f92ba60697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e03faa6-48d7-40d5-a787-a019d576b713",
   "metadata": {},
   "source": [
    "## Load PDF from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10e2cd-bbb3-4629-8694-e9f719dd553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define PDF file to read\n",
    "#\n",
    "file_path = \"./documents/bitcoin.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede7a36-d82c-4b3c-b7e6-51853b066e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load content of PDF file into memory\n",
    "#\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38d29b-1647-4d52-b776-4a675a225b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path,mode=\"single\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707454b3-b037-4d97-b8b2-5a5578dbfca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7543fd-d05a-4838-b99d-62ba553523d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check content of pages\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bd45eb-6cdc-4063-babf-bb8969b4cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(f\"{pages[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5eb75c-4816-4ce3-9c5d-e4d4fbf4ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb778f-e064-4f93-8a25-c93726bc5eda",
   "metadata": {},
   "source": [
    "## Text Splitter\n",
    "\n",
    "A text splitter breaks a text into smaller pieces. Those smaller parts are then loaded into the vector database together with their embeddings. There are many different text splitting strategies. The best ones are observing the content (semantic splitters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d000a5f0-e691-4ab8-bc44-590d47a820b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521a61c-3eea-4545-ae7d-3ee3de00ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1008636-a7b8-429e-98c7-a3717db54e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9f872-bb6b-4d5a-ab8a-d92da60030da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401c04b-35ca-46e0-a516-31dd50fb225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924ace3-1cf1-4f42-a5a2-58f3b50b8f29",
   "metadata": {},
   "source": [
    "## Create Vectorstore and fill it\n",
    "\n",
    "A vector store is a database which stores information together with the embedding of the information. In our case it is a memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797bc3b-1c95-4e2d-b850-6a1df9b0d8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423164d-d473-4f1e-86ef-003be70f5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This functions allows us to search for similar text pieces in the vector store. \n",
    "# We give it a query, it calculates the embedding and then searches in the embeddings for similar entires and returns the text of the entry.\n",
    "#\n",
    "docs = vector_store.similarity_search(\"What is Proof-of-work?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665066e-71f6-4ec7-8066-7cc19ae656c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Show all found similar text parts\n",
    "#\n",
    "for doc in docs:\n",
    "    pprint.pp(f'Doc {doc}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3425e4c-a796-4613-ad1f-aaf026b5e8b8",
   "metadata": {},
   "source": [
    "## Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24704870-3464-4d5c-bac7-2f4e44cc7222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76837448-7720-4596-9a8f-b5db5c0c7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e338f-444a-458d-872a-7848ffabc685",
   "metadata": {},
   "source": [
    "## Setup Simple Sequence Solution\n",
    "\n",
    "Here we simulate a RAG setup by executing just the required steps one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f9da8-e132-4db6-a68a-f9878d30ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc67b4-b666-4a16-81b6-3a8d638384ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# User query (R8)\n",
    "#\n",
    "query = \"What is proof-of-work in the context of bitcoin?\" # simple query\n",
    "#query = \"What is the title of the work of W. Feller?\" # complex query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35072c27-6c71-4111-9784-e65ac4e8067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Collect supporting information from database\n",
    "#\n",
    "retrieved_docs = vector_store.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd40f733-2165-43a8-b34f-338b8082b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Convert found documents from database\n",
    "#\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331237b3-96fd-4590-8d7c-3bc57964f07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d392e395-92eb-4dec-8813-14b7e7c65bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\" \n",
    "You are a helpful assistant answering questions from users.\n",
    "\n",
    "You have the following context information for answering the user query:\n",
    "-------------------------------------\n",
    "{context}\n",
    "-------------------------------------\n",
    "\n",
    "Now, please answer the following user question based on the context information. \n",
    "Do not use your own knowledge.\n",
    "If the context information is not sufficient, do not answer the query.\n",
    "\n",
    "User query:\n",
    "{query}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d094f0d-920b-4b66-bddd-ed7c2334ea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9d1aa5-d054-43ec-a48f-92e6cdd446fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Setup prompt\n",
    "#\n",
    "messages = prompt.invoke({\"query\": query, \"context\": context})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75293d-0c5e-46f0-8b90-61d1ce764f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Call LLM\n",
    "#\n",
    "output = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f549a2b-ba4b-4ef4-bc0e-b009ccf2c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9adca4-b57d-4e98-8a74-aff4fe120f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check if the context really helped. Make the same query to the LLM, but remove the context.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477e437-278f-4da1-a246-11087af83cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.invoke({\"query\": query, \"context\": '' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd38da8e-ece9-48e6-bfc3-0322e39f625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed93f0da-e81e-4395-b442-ce3b7a3c8cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285ec4b-cbd0-4a30-b2a9-aedad6a8b828",
   "metadata": {},
   "source": [
    "## Setup Agentic Solution\n",
    "\n",
    "We are using now the agent framework of langchain, langgraph to setup an agent which can do RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3db250-8e38-430d-96ff-c06ee93fc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba81854-c0eb-402a-8207-cecfc3ca594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"query\": state[\"question\"], \"context\": docs_content})\n",
    "    system_prompt\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea3a2e-a89d-4484-9afb-aa059c000b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is Proof-of-work?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a0678-b82e-4617-9c69-7d7b0c36ecce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e4765a-2905-4716-9710-67f411ef7b3e",
   "metadata": {},
   "source": [
    "# A better solution\n",
    "\n",
    "RAG suffers from the problem that text parts often miss the context and that a question does not always have a similar embedding than the answer. There are ways to soften those problems. \n",
    "\n",
    "- Add context to each text part (e.g. a summary of the complete chapter)\n",
    "- Use better text splitting concepts (e.g. semantic chunking)\n",
    "- Rewrite the user query if no suitable text part is found (ReAct)\n",
    "- Combine retrieval strategies (e.g. add key word based retrievers)\n",
    "\n",
    "Lets try the ReAct (rethink and act) approach. This agent includes a reflection step which checks the intermediate results and rewrites the query in case of a poor retrieval quality.\n",
    "A detailed solution can be found at https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_agentic_rag.ipynb.\n",
    "\n",
    "The following adaptations are required:\n",
    "\n",
    "- change the LLMs (one LLM init is sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272ab7b2-459e-4eb9-8d36-e93fa61c4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
