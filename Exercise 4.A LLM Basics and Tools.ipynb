{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c54c5f04-16c9-4bc8-8458-3b870044f330",
   "metadata": {},
   "source": [
    "![ALT_TEXT_FOR_SCREEN_READERS](./header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bdb1b5-979d-4b1a-b26e-1bd48859d405",
   "metadata": {},
   "source": [
    "# Exercise 4.A LLM Basics and Tools\n",
    "\n",
    "The goal of this exercise is to setup a connection from the notebook to a local or remote large language model (LLM). Using this connection\n",
    "the basic modes of working with LLMs shall be tested:\n",
    "- Text Completion: complete a text. A query text is given and the model continues this text.\n",
    "- Chat: A list of system and user messages is given and the model generates the next message.\n",
    "\n",
    "We are using **OLLAMA**[1] for local execution of the LLM and the framework **langchain**[2] for the access to the model.\n",
    "\n",
    "- [1] [https://ollama.com/](https://ollama.com/)\n",
    "- [2] [https://www.langchain.com/](https://www.langchain.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ab026-4589-42e9-b8dc-1c8c689485de",
   "metadata": {},
   "source": [
    "# Considerations\n",
    "\n",
    "- Read the tutorial carefully\n",
    "- Install OLLAMA [1] on your machine\n",
    "- Start with the model llama3.2 https://ollama.com/library/llama3.2 and try to serve it using ollama\n",
    "- Use ```ollama pull <name-of-model>``` to load a model from the server to a local memory. Take care for the memory footprint on your machine.\n",
    "- Install **langchain-ollama** and **langchain** packages into your environment (use pip inside the workbook and comment it out later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062af60-c1a6-45f6-ad04-4fa4a3e6ce40",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- R0: Instantiate a text completion model object and improve the head of the story for completion (10%)\n",
    "- R1: Instantiate a chat completion model object and work to improve the system prompt message (10%)\n",
    "- R2: Test the effect of the temperature parameter values 0.0 and 1.0 in the chat completion example (20%)\n",
    "- R3: Extend the structured output of the joke class (20%)\n",
    "- R4: Check the produced tool message for correct parameters (10%)\n",
    "- R5: Extend the tool calling agent by a system date and time tool (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c81e1-b46f-4c1d-a65e-73026116b3c7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02d7a97-5801-4d83-8070-ee6169b88d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9898a6c-7d46-43f1-a121-4d898052b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa52684-85c1-498b-9260-3ec9c75672b4",
   "metadata": {},
   "source": [
    "# Text Completion\n",
    "\n",
    "Text completion means that the LLM takes the given text as start of a story and continues to generate tokens which extend the story.\n",
    "Such a model interface takes a string as input and generates a string as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063a4ed0-4459-473e-a713-eeb10c574f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c207790-932c-4751-a55a-d4e56d416787",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83457efe-3a93-4bc5-831e-1c848571ba25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...where the misty dawn broke through the jagged peaks of the Alps, a small village lay shrouded in mystery and legend. The villagers, clad in traditional Tyrolean attire, lived simple lives amidst the rugged beauty of their surroundings, their days filled with hard work and quiet traditions.\n",
      "\n",
      "In this remote valley, the air was crisp and clean, filled with the scent of pine trees and the sound of rushing water. A narrow stream ran through the village, its crystal clear waters reflecting the vibrant colors of the surrounding mountains.\n",
      "\n",
      "As you walked through the village, you would notice a sense of unease in the air. The villagers were wary of outsiders, their faces stern and guarded. They lived by the old ways, with ancient customs and superstitions that had been passed down through generations.\n",
      "\n",
      "Despite the eerie atmosphere, there was something hauntingly beautiful about this dark Tyrolean valley. A sense of enchantment hung over it like a shroud, beckoning in those brave enough to explore its secrets.\n",
      "\n",
      "And then, one day, you stumbled upon an ancient wooden door hidden behind a waterfall. The door creaked ominously as you pushed it open, revealing a dimly lit tunnel that seemed to lead into the very heart of the valley itself...\n",
      "\n",
      "Would you like to continue exploring this dark and mysterious world?\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# R0: improve the start of the story to resemble your favorite fairy tale...\n",
    "#\n",
    "result = llm.invoke('Once upon a time, in a dark tyrolian valley ')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041175af-c5fc-41fc-ae3b-469a69e60966",
   "metadata": {},
   "source": [
    "# Chat Completion\n",
    "\n",
    "A chat completion model takes a list of messages as input and generates the next message. The resulting message is of type **AIMessage**.\n",
    "The list of messages can contain tuples with role and prompt for different message objects from the langchain_core.messages module.\n",
    "\n",
    "**Note:** The chat model is a different interface compared to the completion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d7a4d5-9377-4b05-a285-bdba55302da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4f695-89c5-4706-8bca-4c59b0d1271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef6963eb-566b-4d21-afab-7c677713befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R1: improve the system prompt such that it contains all elements as learned in the slides.\n",
    "#\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a skilled philospher. You can explain complext information in simple terms.\" ),\n",
    "    (\"human\",  \"What is the meaning of life?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00b9122-d310-43e0-9f62-b9812ce039a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The age-old question that has puzzled humans for centuries! The meaning of life is a complex and multifaceted concept, but I'll try to break it down in simple terms.\n",
      "\n",
      "At its core, the meaning of life refers to our purpose, direction, or significance. It's the reason why we exist, what drives us, and how we want to be remembered when we're gone. But here's the thing: there is no one definitive answer to this question. Different people find meaning in different things.\n",
      "\n",
      "Some people might say that the meaning of life is to seek happiness, love, or fulfillment. Others might argue that it's to pursue knowledge, creativity, or spiritual growth. Perhaps for some, it's about leaving a lasting legacy or making a positive impact on the world.\n",
      "\n",
      "The truth is, our lives are made up of many different aspects, and finding meaning can be a highly personal and subjective experience. It's like trying to assemble a puzzle with many different pieces – each one has its own unique shape and fit.\n",
      "\n",
      "One way to think about it is this: what gives your life significance? Is it the relationships you have, the work you do, or the experiences you have? Maybe it's a combination of these things. For some people, meaning comes from within – they find purpose in their own personal growth, self-discovery, and understanding.\n",
      "\n",
      "Another way to look at it is this: what do you want to leave behind when you're gone? Do you want to be remembered for your kindness, generosity, or bravery? Or perhaps you just want to have lived a life that was true to yourself?\n",
      "\n",
      "Ultimately, the meaning of life is not something we can definitively answer. It's more like an ever-evolving question – one that changes as we grow, learn, and experience new things.\n",
      "\n",
      "But here's the good news: regardless of what our lives mean, they are all precious and valuable in their own way. Every moment, every decision, and every connection we make has the potential to bring meaning and purpose into our existence.\n",
      "\n",
      "So, what do you think? What gives your life meaning?\n"
     ]
    }
   ],
   "source": [
    "# call the LLM with the message\n",
    "ai_msg = llm.invoke(messages)\n",
    "print(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bf2ba92-738c-4702-b5a2-f50aadd8e8b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# specific return value\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mai_msg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "# specific return value\n",
    "print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2236b-c06b-42d6-810a-5ab8383c4404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other metadata\n",
    "print(ai_msg.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe23026-b446-421d-84a8-a9d92641f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R2: test the impact of the temperature parameter (1.0) using the same prompt.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ade5d2-ac9a-4d45-961e-ba450f06128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76ea9f3e-a189-4121-9bd1-38a930ef029f",
   "metadata": {},
   "source": [
    "# Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ab15ba-e350-4b55-a2c3-453f43fe0fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2aa48132-51c6-4da6-814b-abb40e1d7b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    explanation: str = Field(description=\"Detailed explanation why the joke if funny.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d78861de-169d-48a5-bf98-5f5c8fce3d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\",temperature=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3df54425-bd85-4f3e-be86-bfbff4c7617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(Joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "529a30f7-80ff-4904-bc40-4dd36a38015b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the AI go to therapy?', punchline='It had a little bug to work through.', explanation='The setup establishes an expectation of something serious, but the punchline takes an unexpected twist.')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm.invoke(\"Tell me a joke about AIs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d971280-1ccd-46a1-aefe-45635014363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R3: extend the structured output by an explanation field which contains the reasoning, why this is actually funny.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd8ec7-aede-4312-ad4c-d009c3f9f1de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f6c668-fcdb-4324-b906-3c799a3a3e3e",
   "metadata": {},
   "source": [
    "# Tool use\n",
    "\n",
    "Tool use is a way to extend the function of LLMs. This is done in the following steps:\n",
    "1. in the first step, the tools are defined and the definitions are passed to the LLM as context information. The LLM is instructed to extract optional tool use, including tool name and tool parameters, if the user query requests a tool use. In this case the LLM generates a tool use message with all required parameters.\n",
    "1. in the second step, a piece of application logic takes the tool call messages and executes the tool calls. The results are placed back in the message list.\n",
    "1. in the last step, the tool results are used to formulation the answer.\n",
    "\n",
    "Those steps are usually stitched together in an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c443c62d-c8c1-4aca-a92c-0ee25dda6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e171c51e-0e45-4066-8110-bd3604bf3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# @tool is an annotator which adds specific properties to your tool function. Those properties allow the LLM to understand the parameters and the output of the tool.\n",
    "#\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76d4d1a2-390f-40e6-a9c3-611aacb07044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def square_root(x: float) -> float:\n",
    "    \"\"\"returns the square root of x.\"\"\"\n",
    "    return math.sqrt(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "426eec74-4ec4-45f2-87d4-f99bc2b23dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# call tool for testing\n",
    "print(multiply.invoke({\"first_int\": 4, \"second_int\": 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c383956a-afc2-4b31-abcb-152ee215051d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for multiply\n",
      "first_int\n",
      "  Input should be a valid integer, got a number with a fractional part [type=int_from_float, input_value=4.1, input_type=float]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/int_from_float\n"
     ]
    }
   ],
   "source": [
    "# call tool with wrong parameters for testing\n",
    "try:\n",
    "    print(multiply.invoke({\"first_int\": 4.1, \"second_int\": 5}))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f72e1786-5e7d-4f76-841f-336bbe638ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print(square_root.invoke({'x':25.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a28058-1e2b-4768-8645-2d6fed258aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fdd0433-d1ac-4b22-9473-628e8f8aeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare LLM for tool use\n",
    "#\n",
    "tools = [multiply,square_root]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db7385bc-485f-47d3-94b9-b2604298f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c6d9995-4616-4fea-a724-bcff3e1a90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# bind tools to LLM\n",
    "#\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4456c55-4ecc-412e-94e0-8396c21f5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'what is ten times 7ish?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "28707264-8395-4a64-bb42-af485c97c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R4: check the resulting tool message for the correct tool parameters. Extract the tool calling values (tool name, tool parameters). Those parameter can be used to call the python tool.\n",
    "#\n",
    "result = llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ee9368f-16a4-428c-ae3d-990f5d839a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-29T13:52:02.321474Z', 'done': True, 'done_reason': 'stop', 'total_duration': 770661000, 'load_duration': 28722333, 'prompt_eval_count': 216, 'prompt_eval_duration': 124638042, 'eval_count': 24, 'eval_duration': 616690666, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-9f8cc439-fe67-4673-930c-818c8385af77-0', tool_calls=[{'name': 'multiply', 'args': {'first_int': '10', 'second_int': '7'}, 'id': '8dc5246f-413d-41b7-95de-c896689edfa0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 216, 'output_tokens': 24, 'total_tokens': 240})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "319caba3-264f-4c56-bf4a-620b96290c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'first_int': '10', 'second_int': '7'},\n",
       "  'id': '8dc5246f-413d-41b7-95de-c896689edfa0',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaea2c-525d-4186-8206-738238873a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743b9c3b-4aa5-4ca8-8f28-b241c5aad38a",
   "metadata": {},
   "source": [
    "# Tool calling agent\n",
    "\n",
    "The langchain tool calling agent is a class which integrates all three steps of tool calling into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01b35e7e-da01-40cb-9894-616aa852373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93dc6844-3ca8-49de-bb6f-3b008aa970a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you're a helpful calculation assistant. Use your tools where it is appropriate. Mimic the style of the user query.\"), \n",
    "    (\"human\", \"{input}\"), \n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b894621d-4287-4357-92ab-d3167fbec48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6d41447-b25d-433a-a7ff-147a3c004b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0a6b07b2-91f1-47a4-9b33-77e3428010da",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34540d74-8b37-440c-be34-1589cf75be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `multiply` with `{'first_int': '10', 'second_int': '7'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m70\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `square_root` with `{'x': '70'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m8.366600265340756\u001b[0m\u001b[32;1m\u001b[1;3mThe square root of ten times 7 is approximately 8.37.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'what is the square root of ten times 7ish?', 'output': 'The square root of ten times 7 is approximately 8.37.'}\n"
     ]
    }
   ],
   "source": [
    "print(agent_executor.invoke({\"input\": \"what is the square root of ten times 7ish?\", }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa566f9-f348-4b33-ad5c-fa1c29fe674b",
   "metadata": {},
   "source": [
    "# Agent with system time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f060be2-07c3-482e-bb70-ca384bf0be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# R5: extend the tool calling agent by a tool which returns the current date and time as string. The LLMs usually do now know date and time!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3dfd2e-9623-4edc-a329-318620b21f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b157d512-8e70-4433-99a6-d818426c6a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
